# Active Neural Localization (2018)

Devendra Singh Chaplot, Emilio Parisotto, Ruslan Salakhutdinov

---

Localization is the problem of estimating the location of an autonomous agent from an observation and the map of the environment. The ability to localize under uncertainity is required by autonomous agents to perform various downstream tasks such as planning, exploration and target navigation. They tackle the task of *global* localization, where the initial position of the agent is unknown. The other type of localization task is *local* localization, where the initial position of the agent is known, and the agent must track its position as it moves.

Many of the other localization approaches are *passive*, meaning the agent tries to locate itself from observations alone. Their work is *active*, meaning the agent can also take actions. They found that using an active agent gives both faster and more accurate localization. Their agent uses a raw pixel observation and a map of the environment, and they test their implementation using both a 2D maze and the 3D VizDoom environment.

Their work is based on Baysian filtering, which uses *belief* The belief is the probability distribution overall all locations which is predicted using the previous observation, previous action and the *likelihood*. The likelihood is the probability of receiving an observation if you are in a location, e.g. if you have an environment with a red and a blue room and all you can see (your observation) is red, then your likelihood over locations in the red room are high and those in the blue room are low. The likelihood is given by a *perceptual model* which uses the previous predicted location and previous action. Belief at the first time-step is known as the *prior*. For global localization, you aren't given your starting location, so the prior is uniform over all possible locations.

They represent the location as a three element tuple of x-position, y-position and orientation. The belief and likelihood are represented by three dimensional tensors of shape [X, Y, O], i.e. the x-coordinate, the y-coordinate and the orientation. If you had a 10x10 grid with 4 orientations (north, south, east and west) then the tensors would be [10, 10, 4]. They refer to these tensors as *maps*. They always use 4 orientations in their experiments, with their agents in 3D environments only able to turn 90 degrees.

The architecture, shown in figure 1, has *perceptual model* and a *policy model*. The perceptual model takes in the observation and outputs a likelihood map. The likelihood map is then element-wise multiplied with the belief map from the previous time-step (passed through a transition function) and then scaled by a normalization constant - the sum of the likelihood and previous belief maps for each possible location for this observation - to get the belief map. The belief map is then fed to the policy model to take an action. The transition function is used to update the belief map, depending on the action taken, which will be used by the likelihood map in the next time-step. See appendix B for more about the transition function.

The perceptual model computes the features from the observation and state given by the map of the environment. The likelihood of each state is given by the cosine similarity between the features from the observation and the features of the state. 

For 2D maze environments, the observation is used to make a one-hot feature vector (deterministically, not with a neural network). The mazes are represented by a binary matrix, 0 denoting a wall and 1 a free space. The agent's observation is the series of pixels in front of the agent, with all pixels after the first wall being obstructed and taking a value of 0. They try with 7x7, 15x15 and 21x21 mazes with different episode lengths.

For 3D environments, the observation is fed through a convolutional neural network (CNN). The x and y coordinates are continuous, by discretized into discrete values. This means the beliefs are actually a small range of locations. At the start of each episode the agent is spawned at a random location in this continuous range. They also state the agent gets extra observations from certain locations in all 4 orientations. I don't understand why they use this extra observations, but they say it is necessary to use them. They experiment using a 3D maze in VizDoom, where textures can be randomized, and Unreal3D which is a photo-realistic environment that looks like a modern office. The mazes are 70x70 and the office is 70x50. They also add a small amount of noise to each transition. They test the maze with "landmarks", which are single walls with unique textures that are randomized throughout training. The more landmarks, the easier it is to navigate. They test the mazes with unseen maps with seen textures and unseen maps with unseen textures. They do a similar thing with Unreal3D by training on maps with the lights turned on and then testing their agent on the same maps with the lights turned off. The agent doesn't seem to generalize from lights on -> lights off. They also test a model trained on the 3D maze on Unreal3D (with lights on) and find it does pretty good.

The policy model is trained with A3C. At each time-step the agent receives a reward equal to the maximum probability (from the belief map) of being in a state (even if it's not the correct state). This reduces the entropy of the belief map which causes the agent to localize as fast as possible. They found that although you don't need to do this, it reduces training time. At the end of an episode, the prediction is the state with the highest probability in the belief map. The agent receives a reward (+1) if this was the correct location. Their metric is accuracy, the ratio of the agent correctly predicting its own location at the end of an episode over all episodes.

Figure 4 is a great visualization of how the agent localizes itself. There is some implementation details in appendix B.

---

https://github.com/devendrachaplot/Neural-Localization